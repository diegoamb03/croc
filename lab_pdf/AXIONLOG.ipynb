{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa282bfd",
   "metadata": {},
   "source": [
    "## Axion log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345f6468",
   "metadata": {},
   "source": [
    "#### Transacciones PDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "17e1d7ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pymupdf in c:\\users\\analista de  datos\\appdata\\roaming\\python\\python312\\site-packages (1.26.0)\n",
      "Requirement already satisfied: pdfplumber in c:\\users\\analista de  datos\\appdata\\roaming\\python\\python312\\site-packages (0.11.6)\n",
      "Requirement already satisfied: tabula-py in c:\\users\\analista de  datos\\appdata\\roaming\\python\\python312\\site-packages (2.10.0)\n",
      "Requirement already satisfied: pytesseract in c:\\users\\analista de  datos\\appdata\\roaming\\python\\python312\\site-packages (0.3.13)\n",
      "Requirement already satisfied: Pillow in c:\\users\\analista de  datos\\appdata\\roaming\\python\\python312\\site-packages (11.2.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\analista de  datos\\appdata\\roaming\\python\\python312\\site-packages (2.2.3)\n",
      "Requirement already satisfied: camelot in c:\\users\\analista de  datos\\appdata\\roaming\\python\\python312\\site-packages (12.6.29)\n",
      "Requirement already satisfied: camelot-py[cv] in c:\\users\\analista de  datos\\appdata\\roaming\\python\\python312\\site-packages (1.0.0)\n",
      "Requirement already satisfied: pdfminer.six==20250327 in c:\\users\\analista de  datos\\appdata\\roaming\\python\\python312\\site-packages (from pdfplumber) (20250327)\n",
      "Requirement already satisfied: pypdfium2>=4.18.0 in c:\\users\\analista de  datos\\appdata\\roaming\\python\\python312\\site-packages (from pdfplumber) (4.30.1)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in c:\\users\\analista de  datos\\appdata\\roaming\\python\\python312\\site-packages (from pdfminer.six==20250327->pdfplumber) (3.4.1)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in c:\\users\\analista de  datos\\appdata\\roaming\\python\\python312\\site-packages (from pdfminer.six==20250327->pdfplumber) (45.0.3)\n",
      "Requirement already satisfied: numpy>1.24.4 in c:\\users\\analista de  datos\\appdata\\roaming\\python\\python312\\site-packages (from tabula-py) (2.2.5)\n",
      "Requirement already satisfied: distro in c:\\users\\analista de  datos\\appdata\\roaming\\python\\python312\\site-packages (from tabula-py) (1.9.0)\n",
      "Requirement already satisfied: click>=8.0.1 in c:\\users\\analista de  datos\\appdata\\roaming\\python\\python312\\site-packages (from camelot-py[cv]) (8.2.1)\n",
      "Requirement already satisfied: chardet>=5.1.0 in c:\\users\\analista de  datos\\appdata\\roaming\\python\\python312\\site-packages (from camelot-py[cv]) (5.2.0)\n",
      "Requirement already satisfied: openpyxl>=3.1.0 in c:\\users\\analista de  datos\\appdata\\roaming\\python\\python312\\site-packages (from camelot-py[cv]) (3.1.5)\n",
      "Requirement already satisfied: pypdf<6.0,>=4.0 in c:\\users\\analista de  datos\\appdata\\roaming\\python\\python312\\site-packages (from camelot-py[cv]) (5.5.0)\n",
      "Requirement already satisfied: tabulate>=0.9.0 in c:\\users\\analista de  datos\\appdata\\roaming\\python\\python312\\site-packages (from camelot-py[cv]) (0.9.0)\n",
      "Requirement already satisfied: opencv-python-headless>=4.7.0.68 in c:\\users\\analista de  datos\\appdata\\roaming\\python\\python312\\site-packages (from camelot-py[cv]) (4.11.0.86)\n",
      "Requirement already satisfied: packaging>=21.3 in c:\\users\\analista de  datos\\appdata\\roaming\\python\\python312\\site-packages (from pytesseract) (25.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\analista de  datos\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\analista de  datos\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\analista de  datos\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: SQLAlchemy<0.8.0,>=0.7.7 in c:\\users\\analista de  datos\\appdata\\roaming\\python\\python312\\site-packages (from camelot) (0.7.10)\n",
      "Requirement already satisfied: Elixir>=0.7.1 in c:\\users\\analista de  datos\\appdata\\roaming\\python\\python312\\site-packages (from camelot) (0.7.1)\n",
      "Requirement already satisfied: sqlalchemy-migrate>=0.7.1 in c:\\users\\analista de  datos\\appdata\\roaming\\python\\python312\\site-packages (from camelot) (0.11.0)\n",
      "Requirement already satisfied: Jinja2>=2.5.5 in c:\\users\\analista de  datos\\appdata\\roaming\\python\\python312\\site-packages (from camelot) (3.1.6)\n",
      "Requirement already satisfied: xlwt==0.7.2 in c:\\users\\analista de  datos\\appdata\\roaming\\python\\python312\\site-packages (from camelot) (0.7.2)\n",
      "Requirement already satisfied: xlrd==0.7.1 in c:\\users\\analista de  datos\\appdata\\roaming\\python\\python312\\site-packages (from camelot) (0.7.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\analista de  datos\\appdata\\roaming\\python\\python312\\site-packages (from click>=8.0.1->camelot-py[cv]) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\analista de  datos\\appdata\\roaming\\python\\python312\\site-packages (from Jinja2>=2.5.5->camelot) (3.0.2)\n",
      "Requirement already satisfied: et-xmlfile in c:\\users\\analista de  datos\\appdata\\roaming\\python\\python312\\site-packages (from openpyxl>=3.1.0->camelot-py[cv]) (2.0.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\analista de  datos\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: pbr>=1.8 in c:\\users\\analista de  datos\\appdata\\roaming\\python\\python312\\site-packages (from sqlalchemy-migrate>=0.7.1->camelot) (6.1.1)\n",
      "Requirement already satisfied: decorator in c:\\users\\analista de  datos\\appdata\\roaming\\python\\python312\\site-packages (from sqlalchemy-migrate>=0.7.1->camelot) (5.2.1)\n",
      "Requirement already satisfied: sqlparse in c:\\users\\analista de  datos\\appdata\\roaming\\python\\python312\\site-packages (from sqlalchemy-migrate>=0.7.1->camelot) (0.5.3)\n",
      "Requirement already satisfied: Tempita>=0.4 in c:\\users\\analista de  datos\\appdata\\roaming\\python\\python312\\site-packages (from sqlalchemy-migrate>=0.7.1->camelot) (0.6.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\analista de  datos\\appdata\\roaming\\python\\python312\\site-packages (from cryptography>=36.0.0->pdfminer.six==20250327->pdfplumber) (1.17.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\analista de  datos\\appdata\\roaming\\python\\python312\\site-packages (from pbr>=1.8->sqlalchemy-migrate>=0.7.1->camelot) (80.0.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\analista de  datos\\appdata\\roaming\\python\\python312\\site-packages (from cffi>=1.14->cryptography>=36.0.0->pdfminer.six==20250327->pdfplumber) (2.22)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: camelot-py 1.0.0 does not provide the extra 'cv'\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install pymupdf pdfplumber tabula-py camelot-py[cv] pytesseract Pillow pandas camelot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "17bd7838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lectura de pdf\n",
    "import fitz  \n",
    "import pdfplumber\n",
    "import tabula\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import io\n",
    "import re\n",
    "import pandas as pd\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "42b2d16d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analizando: resOurces/AXION_LOG.pdf\n",
      "Análisis de contenido: {'tiene_texto': True, 'tiene_imagenes': True, 'posibles_tablas': True, 'es_escaneado': False, 'calidad_texto': 'alta'}\n",
      "Extrayendo texto...\n",
      "Extrayendo tablas...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "class PDFExtractorAdaptativo:\n",
    "    def __init__(self):\n",
    "        self.metodos_texto = ['pdfplumber', 'pymupdf', 'ocr']\n",
    "        self.metodos_tablas = ['tabula', 'pdfplumber']  # Quitamos 'camelot'\n",
    "    \n",
    "    def detectar_tipo_contenido(self, archivo_pdf):\n",
    "        \"\"\"Detecta qué tipo de contenido tiene el PDF\"\"\"\n",
    "        try:\n",
    "            doc = fitz.open(archivo_pdf)\n",
    "            primera_pagina = doc[0]\n",
    "            \n",
    "            texto = primera_pagina.get_text().strip()\n",
    "            imagenes = primera_pagina.get_images()\n",
    "            \n",
    "            with pdfplumber.open(archivo_pdf) as pdf:\n",
    "                page = pdf.pages[0]\n",
    "                tablas_detectadas = page.find_tables()\n",
    "            \n",
    "            resultado = {\n",
    "                'tiene_texto': len(texto) > 50,\n",
    "                'tiene_imagenes': len(imagenes) > 0,\n",
    "                'posibles_tablas': len(tablas_detectadas) > 0,\n",
    "                'es_escaneado': len(texto) < 50 and len(imagenes) > 0,\n",
    "                'calidad_texto': 'alta' if len(texto) > 200 else 'baja'\n",
    "            }\n",
    "            \n",
    "            doc.close()\n",
    "            return resultado\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {'error': str(e)}\n",
    "    \n",
    "    def extraer_texto_robusto(self, archivo_pdf):\n",
    "        \"\"\"Extrae texto probando diferentes métodos\"\"\"\n",
    "        metodos_resultados = {}\n",
    "        \n",
    "        # pdfplumber\n",
    "        try:\n",
    "            with pdfplumber.open(archivo_pdf) as pdf:\n",
    "                texto = \"\"\n",
    "                for page in pdf.pages:\n",
    "                    texto += page.extract_text() + \"\\n\"\n",
    "                metodos_resultados['pdfplumber'] = texto\n",
    "        except Exception as e:\n",
    "            metodos_resultados['pdfplumber'] = f\"Error: {e}\"\n",
    "        \n",
    "        # pymupdf\n",
    "        try:\n",
    "            doc = fitz.open(archivo_pdf)\n",
    "            texto = \"\"\n",
    "            for page in doc:\n",
    "                texto += page.get_text() + \"\\n\"\n",
    "            doc.close()\n",
    "            metodos_resultados['pymupdf'] = texto\n",
    "        except Exception as e:\n",
    "            metodos_resultados['pymupdf'] = f\"Error: {e}\"\n",
    "        \n",
    "        # OCR\n",
    "        try:\n",
    "            doc = fitz.open(archivo_pdf)\n",
    "            texto = \"\"\n",
    "            for page_num in range(len(doc)):\n",
    "                page = doc.load_page(page_num)\n",
    "                pix = page.get_pixmap()\n",
    "                img_data = pix.tobytes(\"png\")\n",
    "                img = Image.open(io.BytesIO(img_data))\n",
    "                texto += pytesseract.image_to_string(img, lang='spa') + \"\\n\"\n",
    "            doc.close()\n",
    "            metodos_resultados['ocr'] = texto\n",
    "        except Exception as e:\n",
    "            metodos_resultados['ocr'] = f\"Error: {e}\"\n",
    "        \n",
    "        # Seleccionar mejor resultado\n",
    "        mejor_resultado = \"\"\n",
    "        max_longitud = 0\n",
    "        for metodo, resultado in metodos_resultados.items():\n",
    "            if not resultado.startswith(\"Error\") and len(resultado) > max_longitud:\n",
    "                mejor_resultado = resultado\n",
    "                max_longitud = len(resultado)\n",
    "        \n",
    "        return {\n",
    "            'texto_final': mejor_resultado,\n",
    "            'metodos_intentados': metodos_resultados,\n",
    "            'metodo_exitoso': max([k for k, v in metodos_resultados.items() \n",
    "                                 if not v.startswith(\"Error\") and len(v) == max_longitud], \n",
    "                                default=\"ninguno\")\n",
    "        }\n",
    "    \n",
    "    def extraer_tablas_robusto(self, archivo_pdf):\n",
    "        \"\"\"Extrae tablas probando diferentes métodos (sin camelot)\"\"\"\n",
    "        resultados_tablas = {}\n",
    "        \n",
    "        # tabula-py\n",
    "        try:\n",
    "            tablas = tabula.read_pdf(archivo_pdf, pages='all', multiple_tables=True)\n",
    "            resultados_tablas['tabula'] = {\n",
    "                'tablas': tablas,\n",
    "                'cantidad': len(tablas),\n",
    "                'exito': True\n",
    "            }\n",
    "        except Exception as e:\n",
    "            resultados_tablas['tabula'] = {'error': str(e), 'exito': False}\n",
    "        \n",
    "        # pdfplumber\n",
    "        try:\n",
    "            with pdfplumber.open(archivo_pdf) as pdf:\n",
    "                todas_tablas = []\n",
    "                for page in pdf.pages:\n",
    "                    tablas_pagina = page.extract_tables()\n",
    "                    for tabla in tablas_pagina:\n",
    "                        df = pd.DataFrame(tabla[1:], columns=tabla[0])\n",
    "                        todas_tablas.append(df)\n",
    "                \n",
    "                resultados_tablas['pdfplumber'] = {\n",
    "                    'tablas': todas_tablas,\n",
    "                    'cantidad': len(todas_tablas),\n",
    "                    'exito': True\n",
    "                }\n",
    "        except Exception as e:\n",
    "            resultados_tablas['pdfplumber'] = {'error': str(e), 'exito': False}\n",
    "        \n",
    "        return resultados_tablas\n",
    "    \n",
    "    def procesar_pdf_completo(self, archivo_pdf):\n",
    "        \"\"\"Proceso completo adaptativo\"\"\"\n",
    "        print(f\"Analizando: {archivo_pdf}\")\n",
    "        \n",
    "        info_contenido = self.detectar_tipo_contenido(archivo_pdf)\n",
    "        print(f\"Análisis de contenido: {info_contenido}\")\n",
    "        \n",
    "        resultado_final = {\n",
    "            'archivo': archivo_pdf,\n",
    "            'analisis_contenido': info_contenido,\n",
    "            'texto': None,\n",
    "            'tablas': None\n",
    "        }\n",
    "        \n",
    "        if info_contenido.get('tiene_texto') or info_contenido.get('es_escaneado'):\n",
    "            print(\"Extrayendo texto...\")\n",
    "            resultado_texto = self.extraer_texto_robusto(archivo_pdf)\n",
    "            resultado_final['texto'] = resultado_texto\n",
    "        \n",
    "        if info_contenido.get('posibles_tablas'):\n",
    "            print(\"Extrayendo tablas...\")\n",
    "            resultado_tablas = self.extraer_tablas_robusto(archivo_pdf)\n",
    "            resultado_final['tablas'] = resultado_tablas\n",
    "        \n",
    "        return resultado_final\n",
    "\n",
    "# Ejemplo de uso\n",
    "if __name__ == \"__main__\":\n",
    "    extractor = PDFExtractorAdaptativo()\n",
    "    resultado = extractor.procesar_pdf_completo(\"resOurces/AXION_LOG.pdf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "cbba7dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertir_numero(valor):\n",
    "    # Eliminar espacios\n",
    "    valor = valor.strip()\n",
    "    # Caso coma como decimal\n",
    "    if ',' in valor:\n",
    "        valor = valor.replace('.', '').replace(',', '.')\n",
    "    return float(valor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "ccf13078",
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpiar_columnas(df):\n",
    "    \"\"\"\n",
    "    Limpia los nombres de las columnas de un DataFrame:\n",
    "    - Elimina espacios al inicio y final\n",
    "    - Convierte a minúsculas\n",
    "    - Quita tildes y acentos\n",
    "    - Reemplaza espacios y caracteres especiales por guiones bajos\n",
    "    - Elimina caracteres no alfanuméricos excepto guiones bajos\n",
    "    - Evita guiones bajos duplicados\n",
    "    \"\"\"\n",
    "    def normalizar_texto(texto):\n",
    "        # Quitar tildes y acentos usando unicodedata\n",
    "        texto_sin_acentos = unicodedata.normalize('NFD', texto)\n",
    "        texto_sin_acentos = ''.join(c for c in texto_sin_acentos if unicodedata.category(c) != 'Mn')\n",
    "        return texto_sin_acentos\n",
    "    \n",
    "    df.columns = (\n",
    "        df.columns\n",
    "        .str.strip()                                    # Quitar espacios al inicio y final\n",
    "        .str.lower()                                    # Convertir a minúsculas\n",
    "        .map(normalizar_texto)                          # Quitar tildes y acentos\n",
    "        .str.replace(r'[\\s\\-]+', '_', regex=True)       # Espacios y guiones por guiones bajos\n",
    "        .str.replace(r'[^\\w]', '_', regex=True)         # Caracteres especiales por guiones bajos\n",
    "        .str.replace(r'_+', '_', regex=True)            # Múltiples guiones bajos por uno solo\n",
    "        .str.strip('_')                                 # Quitar guiones bajos al inicio y final\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "878c01a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "#extraer del diccionario final tablas y texto\n",
    "textos = resultado.get('texto', {}).get('texto_final', '')\n",
    "print(type(textos))\n",
    "\n",
    "#Escritura .txt\n",
    "with open(\"AXION_LOG.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(textos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "a141290e",
   "metadata": {},
   "outputs": [],
   "source": [
    "texto = textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "714ad2cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Fecha Orden': '29/04/2025',\n",
       " 'Pedido a': 'CROC SAS',\n",
       " 'Dirección': 'CALLE 69 # 19 -36 BOGOTA',\n",
       " 'Emitido en': 'TENJO',\n",
       " 'Nro Nota Pedido': '655664',\n",
       " 'Condiciones Pago': '45 Días Fecha Factura',\n",
       " 'Última Fecha Entrega': '08/05/2025',\n",
       " 'Destino': '0101 Axis Log. SAS-BOG.'}"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === 1. Extraer encabezado (solo el primero)\n",
    "encabezado = {}\n",
    "encabezado['Fecha Orden'] = re.search(r'Fecha de la Orden\\s+(\\d{2}/\\d{2}/\\d{4})', texto).group(1)\n",
    "encabezado['Pedido a'] = re.search(r'Pedido a\\s+(.*?)\\n', texto).group(1).strip()\n",
    "encabezado['Dirección'] = re.search(r'CALLE [^\\n]+', texto).group(0).strip()\n",
    "encabezado['Emitido en'] = re.search(r'Emitido en\\s+(.*?)\\n', texto).group(1).strip()\n",
    "encabezado['Nro Nota Pedido'] = re.search(r'Nro de Nota de Pedido\\s+(\\d+)', texto).group(1)\n",
    "encabezado['Condiciones Pago'] = re.search(r'Condiciones de Pago:\\s+(.*?)\\s+Ultima Fecha', texto).group(1).strip()\n",
    "encabezado['Última Fecha Entrega'] = re.search(r'Ultima Fecha de Entrega:\\s+(\\d{2}/\\d{2}/\\d{4})', texto).group(1)\n",
    "encabezado['Destino'] = re.search(r'Destino:\\s+([^\\n]+)', texto).group(1).strip()\n",
    "encabezado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247db214",
   "metadata": {},
   "source": [
    "### Escoger Columnas -Ordenar dataframe\n",
    "1. Cliente\n",
    "2. Direccion Cliente\n",
    "3. Destino\n",
    "4. Numero de Pedido\n",
    "5. Fecha de Orden\n",
    "6. Fecha de Entrega\n",
    "7. Id Falso (Para Cruce de Orden de compra )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "25910a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encabezado = pd.DataFrame([encabezado])\n",
    "df_encabezado = limpiar_columnas(df_encabezado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "d3ad3fb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fecha_orden</th>\n",
       "      <th>pedido_a</th>\n",
       "      <th>direccion</th>\n",
       "      <th>emitido_en</th>\n",
       "      <th>nro_nota_pedido</th>\n",
       "      <th>condiciones_pago</th>\n",
       "      <th>ultima_fecha_entrega</th>\n",
       "      <th>destino</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>29/04/2025</td>\n",
       "      <td>CROC SAS</td>\n",
       "      <td>CALLE 69 # 19 -36 BOGOTA</td>\n",
       "      <td>TENJO</td>\n",
       "      <td>655664</td>\n",
       "      <td>45 Días Fecha Factura</td>\n",
       "      <td>08/05/2025</td>\n",
       "      <td>0101 Axis Log. SAS-BOG.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  fecha_orden  pedido_a                 direccion emitido_en nro_nota_pedido  \\\n",
       "0  29/04/2025  CROC SAS  CALLE 69 # 19 -36 BOGOTA      TENJO          655664   \n",
       "\n",
       "        condiciones_pago ultima_fecha_entrega                  destino  \n",
       "0  45 Días Fecha Factura           08/05/2025  0101 Axis Log. SAS-BOG.  "
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_encabezado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "ee478670",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chose Columns\n",
    "#create false columns\n",
    "df_axionlog = df_encabezado.copy()\n",
    "df_axionlog[\"CLIENTE\"] = 'Axionlog'\n",
    "df_axionlog[\"id\"] = 1\n",
    "df_axionlog[\"direccion\"] = df_axionlog[\"destino\"]\n",
    "\n",
    "#renombrar NUMERO_DE_PEDIDO, FECHA_DE_ORDEN, FECHA_DE_ENTREGA, CREAR_ID_FALSO\n",
    "df_axionlog.rename(columns={\n",
    "    'nro_nota_pedido': 'NUMERO_DE_PEDIDO',\n",
    "    'fecha_orden': 'FECHA_DE_ORDEN',\n",
    "    'ultima_fecha_entrega': 'FECHA_DE_ENTREGA',\n",
    "    'direccion': 'DIRECCION',\n",
    "    'destino': 'DESTINO'\n",
    "}, inplace=True)\n",
    "\n",
    "#seleccionar columnas\n",
    "df_axionlog = df_axionlog[['id', 'CLIENTE', 'DIRECCION', 'DESTINO', 'NUMERO_DE_PEDIDO', 'FECHA_DE_ORDEN', 'FECHA_DE_ENTREGA']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "9b0fc92d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>CLIENTE</th>\n",
       "      <th>DIRECCION</th>\n",
       "      <th>DESTINO</th>\n",
       "      <th>NUMERO_DE_PEDIDO</th>\n",
       "      <th>FECHA_DE_ORDEN</th>\n",
       "      <th>FECHA_DE_ENTREGA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Axionlog</td>\n",
       "      <td>0101 Axis Log. SAS-BOG.</td>\n",
       "      <td>0101 Axis Log. SAS-BOG.</td>\n",
       "      <td>655664</td>\n",
       "      <td>29/04/2025</td>\n",
       "      <td>08/05/2025</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id   CLIENTE                DIRECCION                  DESTINO  \\\n",
       "0   1  Axionlog  0101 Axis Log. SAS-BOG.  0101 Axis Log. SAS-BOG.   \n",
       "\n",
       "  NUMERO_DE_PEDIDO FECHA_DE_ORDEN FECHA_DE_ENTREGA  \n",
       "0           655664     29/04/2025       08/05/2025  "
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_axionlog"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e39f96c",
   "metadata": {},
   "source": [
    "#### Productos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "addec82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraer líneas donde comienzan con patrón tipo: \"1.000\\n0\\n496\\nDESCRIPCION...\"\n",
    "patron = re.compile(\n",
    "    r\"(\\d+\\.\\d+)\\s+0\\s+(\\d+)\\s+(.*?)\\s+([\\d.,]+)\\s+(\\w+)\\s+(\\d{2}/\\d{2}/\\d{4})\\s+([\\d.,]+)\\s+([\\d.,]+)\",\n",
    "    re.DOTALL\n",
    ")\n",
    "\n",
    "matches = patron.findall(texto)\n",
    "\n",
    "# Crear DataFrame\n",
    "df = pd.DataFrame(matches, columns=[\n",
    "    'Linea', 'Item', 'Descripción', 'Cantidad',\n",
    "    'Unidad Medida', 'Fecha Entrega', 'Costo Unitario', 'Importe COP'\n",
    "])\n",
    "\n",
    "# Convertir columnas numéricas\n",
    "df['Cantidad'] = df['Cantidad'].apply(convertir_numero)\n",
    "df['Costo Unitario'] = df['Costo Unitario'].apply(convertir_numero)\n",
    "df['Importe COP'] = df['Importe COP'].apply(convertir_numero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "f13a06f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1.000',\n",
       "  '496',\n",
       "  'DON CHICHARRON 35 GR',\n",
       "  '660.0000',\n",
       "  'UN',\n",
       "  '08/05/2025',\n",
       "  '3.166,19',\n",
       "  '2.089.685,40'),\n",
       " ('2.000',\n",
       "  '3892',\n",
       "  'CABANO BAKANO 24GR x 10 UN',\n",
       "  '120.0000',\n",
       "  'PQ',\n",
       "  '08/05/2025',\n",
       "  '23.950,00',\n",
       "  '2.874.000,00'),\n",
       " ('3.000',\n",
       "  '2263',\n",
       "  'DON CHICHARRON ESPIRAL 100GR',\n",
       "  '1,800.0000',\n",
       "  'UN',\n",
       "  '08/05/2025',\n",
       "  '8.460,48',\n",
       "  '15.228.864,00'),\n",
       " ('4.000',\n",
       "  '3083',\n",
       "  'DON CHICHARRONESPIRAL CH 100GR',\n",
       "  '1,500.0000',\n",
       "  'UN',\n",
       "  '08/05/2025',\n",
       "  '1.868,57',\n",
       "  '2.802.855,00'),\n",
       " ('1.000',\n",
       "  '496',\n",
       "  'DON CHICHARRON 35 GR',\n",
       "  '660.0000',\n",
       "  'UN',\n",
       "  '08/05/2025',\n",
       "  '3.166,19',\n",
       "  '2.089.685,40'),\n",
       " ('2.000',\n",
       "  '3892',\n",
       "  'CABANO BAKANO 24GR x 10 UN',\n",
       "  '120.0000',\n",
       "  'PQ',\n",
       "  '08/05/2025',\n",
       "  '23.950,00',\n",
       "  '2.874.000,00'),\n",
       " ('3.000',\n",
       "  '2263',\n",
       "  'DON CHICHARRON ESPIRAL 100GR',\n",
       "  '1,800.0000',\n",
       "  'UN',\n",
       "  '08/05/2025',\n",
       "  '8.460,48',\n",
       "  '15.228.864,00'),\n",
       " ('4.000',\n",
       "  '3083',\n",
       "  'DON CHICHARRONESPIRAL CH 100GR',\n",
       "  '1,500.0000',\n",
       "  'UN',\n",
       "  '08/05/2025',\n",
       "  '1.868,57',\n",
       "  '2.802.855,00'),\n",
       " ('1.000',\n",
       "  '496',\n",
       "  'DON CHICHARRON 35 GR',\n",
       "  '660.0000',\n",
       "  'UN',\n",
       "  '08/05/2025',\n",
       "  '3.166,19',\n",
       "  '2.089.685,40'),\n",
       " ('2.000',\n",
       "  '3892',\n",
       "  'CABANO BAKANO 24GR x 10 UN',\n",
       "  '120.0000',\n",
       "  'PQ',\n",
       "  '08/05/2025',\n",
       "  '23.950,00',\n",
       "  '2.874.000,00'),\n",
       " ('3.000',\n",
       "  '2263',\n",
       "  'DON CHICHARRON ESPIRAL 100GR',\n",
       "  '1,800.0000',\n",
       "  'UN',\n",
       "  '08/05/2025',\n",
       "  '8.460,48',\n",
       "  '15.228.864,00'),\n",
       " ('4.000',\n",
       "  '3083',\n",
       "  'DON CHICHARRONESPIRAL CH 100GR',\n",
       "  '1,500.0000',\n",
       "  'UN',\n",
       "  '08/05/2025',\n",
       "  '1.868,57',\n",
       "  '2.802.855,00')]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "105df2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = limpiar_columnas(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f036404",
   "metadata": {},
   "source": [
    "#### Escoger Columnas Productos \n",
    "1. Numero de item\n",
    "2. descripcion\n",
    "3. Cantidad\n",
    "4. Unidad de medida\n",
    "5. Fecha de entrega\n",
    "6. Costo Unitario\n",
    "7. importe\n",
    "8. id falso (Unir para realizar insersion de datos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "d3404092",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename Columns \n",
    "df_axionlog_productos = df.copy()\n",
    "\n",
    "#rename columns\n",
    "df_axionlog_productos.rename(columns={\n",
    "    'item': 'NUMERO_DE_ITEM',\n",
    "    'descripcion': 'DESCRIPCION',\n",
    "    'cantidad': 'CANTIDAD',\n",
    "    'unidad_medida': 'UNIDAD_DE_MEDIDA',\n",
    "    'fecha_entrega': 'FECHA_ENTREGA',\n",
    "    'costo_unitario': 'COSTO_UNITARIO',\n",
    "    'importe_cop': 'IMPORTE'\n",
    "}, inplace=True)\n",
    "\n",
    "#create false columns\n",
    "df_axionlog_productos[\"id\"] = 1\n",
    "\n",
    "#Select columns\n",
    "df_axionlog_productos = df_axionlog_productos[['id', 'NUMERO_DE_ITEM', 'DESCRIPCION', 'CANTIDAD', 'UNIDAD_DE_MEDIDA', 'COSTO_UNITARIO', 'IMPORTE']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd589935",
   "metadata": {},
   "source": [
    "#### JOIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "815a2746",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge_axionlog = pd.merge(df_axionlog, df_axionlog_productos, on='id', how='inner')\n",
    "\n",
    "#drop duplicates\n",
    "df_merge_axionlog = df_merge_axionlog.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "50cb698b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>CLIENTE</th>\n",
       "      <th>DIRECCION</th>\n",
       "      <th>DESTINO</th>\n",
       "      <th>NUMERO_DE_PEDIDO</th>\n",
       "      <th>FECHA_DE_ORDEN</th>\n",
       "      <th>FECHA_DE_ENTREGA</th>\n",
       "      <th>NUMERO_DE_ITEM</th>\n",
       "      <th>DESCRIPCION</th>\n",
       "      <th>CANTIDAD</th>\n",
       "      <th>UNIDAD_DE_MEDIDA</th>\n",
       "      <th>COSTO_UNITARIO</th>\n",
       "      <th>IMPORTE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Axionlog</td>\n",
       "      <td>0101 Axis Log. SAS-BOG.</td>\n",
       "      <td>0101 Axis Log. SAS-BOG.</td>\n",
       "      <td>655664</td>\n",
       "      <td>29/04/2025</td>\n",
       "      <td>08/05/2025</td>\n",
       "      <td>496</td>\n",
       "      <td>DON CHICHARRON 35 GR</td>\n",
       "      <td>660.0</td>\n",
       "      <td>UN</td>\n",
       "      <td>3166.19</td>\n",
       "      <td>2089685.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Axionlog</td>\n",
       "      <td>0101 Axis Log. SAS-BOG.</td>\n",
       "      <td>0101 Axis Log. SAS-BOG.</td>\n",
       "      <td>655664</td>\n",
       "      <td>29/04/2025</td>\n",
       "      <td>08/05/2025</td>\n",
       "      <td>3892</td>\n",
       "      <td>CABANO BAKANO 24GR x 10 UN</td>\n",
       "      <td>120.0</td>\n",
       "      <td>PQ</td>\n",
       "      <td>23950.00</td>\n",
       "      <td>2874000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Axionlog</td>\n",
       "      <td>0101 Axis Log. SAS-BOG.</td>\n",
       "      <td>0101 Axis Log. SAS-BOG.</td>\n",
       "      <td>655664</td>\n",
       "      <td>29/04/2025</td>\n",
       "      <td>08/05/2025</td>\n",
       "      <td>2263</td>\n",
       "      <td>DON CHICHARRON ESPIRAL 100GR</td>\n",
       "      <td>1.8</td>\n",
       "      <td>UN</td>\n",
       "      <td>8460.48</td>\n",
       "      <td>15228864.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Axionlog</td>\n",
       "      <td>0101 Axis Log. SAS-BOG.</td>\n",
       "      <td>0101 Axis Log. SAS-BOG.</td>\n",
       "      <td>655664</td>\n",
       "      <td>29/04/2025</td>\n",
       "      <td>08/05/2025</td>\n",
       "      <td>3083</td>\n",
       "      <td>DON CHICHARRONESPIRAL CH 100GR</td>\n",
       "      <td>1.5</td>\n",
       "      <td>UN</td>\n",
       "      <td>1868.57</td>\n",
       "      <td>2802855.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id   CLIENTE                DIRECCION                  DESTINO  \\\n",
       "0   1  Axionlog  0101 Axis Log. SAS-BOG.  0101 Axis Log. SAS-BOG.   \n",
       "1   1  Axionlog  0101 Axis Log. SAS-BOG.  0101 Axis Log. SAS-BOG.   \n",
       "2   1  Axionlog  0101 Axis Log. SAS-BOG.  0101 Axis Log. SAS-BOG.   \n",
       "3   1  Axionlog  0101 Axis Log. SAS-BOG.  0101 Axis Log. SAS-BOG.   \n",
       "\n",
       "  NUMERO_DE_PEDIDO FECHA_DE_ORDEN FECHA_DE_ENTREGA NUMERO_DE_ITEM  \\\n",
       "0           655664     29/04/2025       08/05/2025            496   \n",
       "1           655664     29/04/2025       08/05/2025           3892   \n",
       "2           655664     29/04/2025       08/05/2025           2263   \n",
       "3           655664     29/04/2025       08/05/2025           3083   \n",
       "\n",
       "                      DESCRIPCION  CANTIDAD UNIDAD_DE_MEDIDA  COSTO_UNITARIO  \\\n",
       "0            DON CHICHARRON 35 GR     660.0               UN         3166.19   \n",
       "1      CABANO BAKANO 24GR x 10 UN     120.0               PQ        23950.00   \n",
       "2    DON CHICHARRON ESPIRAL 100GR       1.8               UN         8460.48   \n",
       "3  DON CHICHARRONESPIRAL CH 100GR       1.5               UN         1868.57   \n",
       "\n",
       "      IMPORTE  \n",
       "0   2089685.4  \n",
       "1   2874000.0  \n",
       "2  15228864.0  \n",
       "3   2802855.0  "
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mostrar el DataFrame\n",
    "df_merge_axionlog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860197cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#120 * 10 1200 ->Kabano ->Cruce de Numero de Item ->Homologar productos, Cambiar cantidades ->Formato cantidades , ciudad adicional Bogota\n",
    "# Probar otras ordenes "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
